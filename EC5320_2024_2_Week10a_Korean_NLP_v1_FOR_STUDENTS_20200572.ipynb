{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangho24/sogang/blob/main/EC5320_2024_2_Week10a_Korean_NLP_v1_FOR_STUDENTS_20200572.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EC5320 Week10a codes (Korean NLP)\n",
        "\n",
        "2024.11.2.<br>\n",
        "\n",
        "Author: Hyunjoo Yang (hyang@sogang.ac.kr)<br><br>\n",
        "\n",
        "This notebook uses Keras to do sentimental analysis using Korean texts (Natural language processing).<br><br>\n",
        "\n",
        "Main source of codes: <br>\n",
        "https://wikidocs.net/44249 <br><br>\n",
        "\n",
        "Word Cloud codes are from: <br>\n",
        "https://datanavigator.tistory.com/37 <br><br>\n",
        "\n",
        "Data source: <br>\n",
        "https://github.com/e9t/nsmc/ <br><br>\n",
        "\n",
        "If you are interested in Korean NLP, please refer to:<br>\n",
        "https://wikidocs.net/book/2155\n",
        "\n"
      ],
      "metadata": {
        "id": "FAYYy-8VC1uS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install modules"
      ],
      "metadata": {
        "id": "wTDKHJwka2mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Korean NLP moddule\n",
        "\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "hyG7-K6BBIKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k6oX_S9AokO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data loading and preprocessing"
      ],
      "metadata": {
        "id": "ACNs3MTJa_ln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Upload review data"
      ],
      "metadata": {
        "id": "YVlV-u5ZbCOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload review data\n",
        "\n",
        "!unzip korea_nlp.zip"
      ],
      "metadata": {
        "id": "xm34eTWAn3ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_all = pd.read_table('ratings_train.txt')\n",
        "test_data_all = pd.read_table('ratings_test.txt')"
      ],
      "metadata": {
        "id": "7XoCeS7QA8EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련용 리뷰 개수 :',len(train_data_all)) # 훈련용 리뷰 개수 출력"
      ],
      "metadata": {
        "id": "jokh8stvBSbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_all[:10] # 상위 10개 출력"
      ],
      "metadata": {
        "id": "de5v_T_0BUTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('테스트용 리뷰 개수 :',len(test_data_all)) # 테스트용 리뷰 개수 출력"
      ],
      "metadata": {
        "id": "AnommgW7BV_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_all[:10]"
      ],
      "metadata": {
        "id": "zzO06RfZBan_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Sample both train and test data with n=10,000 each"
      ],
      "metadata": {
        "id": "8oSlz0zbShZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data_all.sample(n=10000, random_state=5030)\n",
        "train_data"
      ],
      "metadata": {
        "id": "yaIUug9uSsmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_data_all.sample(n=10000, random_state=5030)\n",
        "test_data"
      ],
      "metadata": {
        "id": "Vvid64sUTAQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. remove English and delete redundant, missing info"
      ],
      "metadata": {
        "id": "sbrtUgwiTO2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# document 열과 label 열의 중복을 제외한 값의 개수\n",
        "train_data['document'].nunique(), train_data['label'].nunique()"
      ],
      "metadata": {
        "id": "i7G3JDwgBbl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# document 열의 중복 제거\n",
        "train_data.drop_duplicates(subset=['document'], inplace=True)"
      ],
      "metadata": {
        "id": "DbN7YbDNBdZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('총 샘플의 수 :',len(train_data))"
      ],
      "metadata": {
        "id": "wOEBGIGzBe1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['label'].value_counts().plot(kind = 'bar')\n"
      ],
      "metadata": {
        "id": "tBhE6qG_Bf14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.groupby('label').size().reset_index(name = 'count'))"
      ],
      "metadata": {
        "id": "4Y0TAtDhBhSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.isnull().values.any())"
      ],
      "metadata": {
        "id": "RoHQL9dgBiPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.isnull().sum())"
      ],
      "metadata": {
        "id": "_8xPzbCYBjWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.loc[train_data.document.isnull()]"
      ],
      "metadata": {
        "id": "9w8E8EysBkVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "metadata": {
        "id": "N2uQUhdJBlc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))"
      ],
      "metadata": {
        "id": "-2zkkakXBnEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 알파벳과 공백을 제외하고 모두 제거\n",
        "eng_text = 'do!!! you expect... people~ to~ read~ the FAQ, etc. and actually accept hard~! atheism?@@'\n",
        "print(re.sub(r'[^a-zA-Z ]', '', eng_text))"
      ],
      "metadata": {
        "id": "TNrYhx7kBoAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글과 공백을 제외하고 모두 제거\n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data[:5]"
      ],
      "metadata": {
        "id": "dt93b9BiBpkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
        "train_data['document'] = train_data['document'].replace('', np.nan)\n",
        "print(train_data.isnull().sum())"
      ],
      "metadata": {
        "id": "WI9KWCadBrqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.loc[train_data.document.isnull()][:5]"
      ],
      "metadata": {
        "id": "N_D276mBBsyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.dropna(how = 'any')\n",
        "print(len(train_data))"
      ],
      "metadata": {
        "id": "BG8ouXBxBvDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
        "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
        "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
        "test_data['document'] = test_data['document'].replace('', np.nan) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
      ],
      "metadata": {
        "id": "Im_JxWUDByCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Tokenization, token indexing, and padding"
      ],
      "metadata": {
        "id": "srvFgX9MbiCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Tokenization"
      ],
      "metadata": {
        "id": "mVJiPTX0cqTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어\n",
        "\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "metadata": {
        "id": "7D2vOZs0Bzoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 토큰화를 위한 형태소 분석기 사용\n",
        "# Open Korean Text (OKT) is an open source Korean tokenizer written in Scala, developed by Will Hohyon Ryu.\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)"
      ],
      "metadata": {
        "id": "D1ywflM9B05O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화와 불용어 제거 진행 (train set)\n",
        "\n",
        "X_train = []\n",
        "for sentence in tqdm(train_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_train.append(stopwords_removed_sentence)"
      ],
      "metadata": {
        "id": "BXCTUGf1B2Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:10]"
      ],
      "metadata": {
        "id": "63bvERTkUTS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[:10])"
      ],
      "metadata": {
        "id": "KPHPnBo-B8ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화와 불용어 제거 진행 (test set)\n",
        "\n",
        "X_test = []\n",
        "for sentence in tqdm(test_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(stopwords_removed_sentence)"
      ],
      "metadata": {
        "id": "e4ZMIDgVB9tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Token indexing"
      ],
      "metadata": {
        "id": "ij5o7vbpcudi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "z_GtVethB_cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('number of word index :', len(tokenizer.word_index))"
      ],
      "metadata": {
        "id": "KwOGi1PPdT_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "qNlFH8ZrCArG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도수가 낮은 단어들은 자연어 처리에서 배제 (등장 빈도수가 3회 미만 확인)\n",
        "\n",
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "metadata": {
        "id": "YvYxPaVCCBf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 단어 개수 중 빈도수 3회 미만인 단어는 제거.\n",
        "# 0번 패딩 토큰을 고려하여 + 1\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ],
      "metadata": {
        "id": "owGOyb_dCEGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draw Word Cloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "fontpath = 'NanumGothic-Regular.ttf'\n",
        "\n",
        "train_word_list = [x for xs in X_train for x in xs]\n",
        "\n",
        "wc = WordCloud(\n",
        "    width = 600,\n",
        "    height = 600,\n",
        "    max_words=20000,\n",
        "    font_path = fontpath,\n",
        "    background_color='white'\n",
        ").generate(str(train_word_list))\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "I128xJizhu5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3회 미만 단어 제거 이후 다시 tokenize\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "77flSzDtCF14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[:10])"
      ],
      "metadata": {
        "id": "4GHgTvBqCHK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save labels as np array\n",
        "\n",
        "y_train = np.array(train_data['label'])\n",
        "y_test = np.array(test_data['label'])"
      ],
      "metadata": {
        "id": "M5HYzYvHCINx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Padding (서로 다른 문장의 길이를 통일)"
      ],
      "metadata": {
        "id": "q0x4MYDAezQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "plt.hist([len(review) for review in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IrCTgKAxCL0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "        count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
      ],
      "metadata": {
        "id": "t8xxmju-CM6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 30\n",
        "below_threshold_len(max_len, X_train)"
      ],
      "metadata": {
        "id": "nfmOQXbbCOcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "QtEQ9ZtiCPzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model train"
      ],
      "metadata": {
        "id": "xAHC-mu7fBN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "XWntS_IRCRBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric to watch during train\n",
        "\"\"\"\n",
        "my_metrics = [tf.keras.metrics.AUC(name='auc'),\n",
        "                tf.keras.metrics.Precision(name='pr'),\n",
        "                tf.keras.metrics.Recall(name='re'),\n",
        "                tf.keras.metrics.BinaryAccuracy(name='ac')\n",
        "                ]\n",
        "\"\"\"\n",
        "my_metrics = [\n",
        "                tf.keras.metrics.BinaryAccuracy(name='ac')\n",
        "                ]"
      ],
      "metadata": {
        "id": "b2yPpF1vWZrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping metric\n",
        "\n",
        "my_monitor = \"val_ac\"\n",
        "#my_monitor = \"val_re\"\n",
        "#my_monitor = \"val_auc\""
      ],
      "metadata": {
        "id": "Ep4ePhJEW-sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping and model check point options\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor=my_monitor, mode='max', patience=15, restore_best_weights=True, verbose=1)\n",
        "mc = tf.keras.callbacks.ModelCheckpoint('best_model.keras', monitor=my_monitor, mode='max', verbose=1, save_best_only=True)"
      ],
      "metadata": {
        "id": "NmgxoDnUW3bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile model and fit\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=my_metrics)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "4BbQjyEKWKu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "elqMoiuffYwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Predictions"
      ],
      "metadata": {
        "id": "5je566_Vg0KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('best_model.keras')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "GPmInNofCTze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
        "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "    score = loaded_model.predict(pad_new) # 예측\n",
        "    score = score[0][0]\n",
        "\n",
        "    if(score > 0.5):\n",
        "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
        "    else:\n",
        "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\n"
      ],
      "metadata": {
        "id": "T-GVc1EtCX9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
      ],
      "metadata": {
        "id": "8z7yZMEDCZko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('이 영화 핵노잼 ㅠㅠ')"
      ],
      "metadata": {
        "id": "I019E7sQCbZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('이딴게 영화냐 ㅉㅉ')"
      ],
      "metadata": {
        "id": "DoRalj1bCcYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('감독 뭐하는 놈이냐?')"
      ],
      "metadata": {
        "id": "oPfh5SYBCdaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('와 쩐다 정말 세계관 최강자들의 영화다')"
      ],
      "metadata": {
        "id": "ZcgD0DZTCecr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}